{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'GeForce RTX 3080'"
      ]
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "source": [
    "import numpy as np\n",
    "import scipy\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import norm\n",
    "\n",
    "import torch\n",
    "device = \"cuda\"\n",
    "torch.cuda.get_device_name(torch.cuda.current_device())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "np.random.seed(0)\n",
    "\n",
    "iris = load_iris()\n",
    "X = iris['data']\n",
    "Y = iris['target']\n",
    "DATA = np.concatenate((X, np.reshape(Y, (len(Y), 1))), axis=1)\n",
    "np.random.shuffle(DATA)\n",
    "training_portion = 0.6\n",
    "\n",
    "index = int(len(DATA) * training_portion)\n",
    "TRAIN = DATA[:index, :]\n",
    "TEST  = DATA[index:, :]\n",
    "\n",
    "Xtrain, Ytrain = TRAIN[:, :-1], TRAIN[:, -1]\n",
    "Xtest , Ytest  = TEST[:, :-1], TEST[:, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "((90, 4), (90,), (60, 4), (60,))"
      ]
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "Xtrain.shape, Ytrain.shape, Xtest.shape, Ytest.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PCA_pvalue import PCA_pvalue\n",
    "from Distance_Classifier import Distance_classifier\n",
    "\n",
    "# m=PCA_pvalue(Xtrain, np.array(Ytrain, dtype=np.int32))\n",
    "# m.fit()\n",
    "# m.all_p_values(Xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(tensor([0.5774, 0.5774, 0.5774], requires_grad=True),\n",
       " tensor([0.6667, 0.6667, 0.6667]))"
      ]
     },
     "metadata": {},
     "execution_count": 33
    }
   ],
   "source": [
    "unit_projection(t)\n",
    "t, t.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unit_projection(t):\n",
    "    with torch.no_grad():\n",
    "        t.set_(t / torch.norm(t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_numpy(x):\n",
    "    \"\"\"Convert a PyTorch tensor to NumPy.\"\"\"\n",
    "    return x.squeeze().detach().cpu().numpy()\n",
    "\n",
    "\n",
    "def to_torch(x, device='cpu'):\n",
    "    return torch.tensor(x).unsqueeze(-1).to(device).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def onehot(labels, uniques=15):\n",
    "    res=torch.zeros((labels.shape[0], uniques), dtype=torch.float64).to(device)\n",
    "    for i in range(res.shape[0]):\n",
    "        res[i,labels[i]] = 1\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[1, 2, 3],\n",
       "        [4, 5, 6]])"
      ]
     },
     "metadata": {},
     "execution_count": 192
    }
   ],
   "source": [
    "torch.tensor([[1,2,3],[4,5,6]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[11.00, 16.00, 21.00],\n",
       "        [ 2.50,  3.50,  4.50]])"
      ]
     },
     "metadata": {},
     "execution_count": 202
    }
   ],
   "source": [
    "torch.matmul(torch.tensor([[3,2],[0.5,0.5]], dtype=torch.float), (torch.tensor([[1,2,3],[4,5,6]], dtype=torch.float)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.tensor([\n",
    "    []\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[ 3,  8],\n",
       "        [ 6, 10],\n",
       "        [ 9, 12]])"
      ]
     },
     "metadata": {},
     "execution_count": 119
    }
   ],
   "source": [
    "torch.transpose(torch.tensor([[1,2,3],[4,5,6]]), 0, 1)*torch.tensor([[3,2],[1,1],[]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([[11., 16., 21.]])\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor(0.)"
      ]
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "source": [
    "def predictpvalue(phat, weight):\n",
    "    return torch.matmul(weight, phat)\n",
    "\n",
    "phat=torch.tensor([[1,2,3],[4,5,6]])\n",
    "weight=torch.tensor([[3,2]])\n",
    "print(predictpvalue(phat, weight).float())\n",
    "\n",
    "torch.nn.MSELoss(reduction='sum')(predictpvalue(phat, weight).float(), torch.tensor([11,16,21]).float())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_printoptions(precision=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Weighted_avg_ensemble:\n",
    "\n",
    "    def __init__(self, classifiers=[Distance_classifier, PCA_pvalue], device='cpu'):\n",
    "        # init classifiers\n",
    "        self.classifiers = classifiers\n",
    "        # init parameters\n",
    "        self.device = device\n",
    "\n",
    "    # @staticmethod\n",
    "    # def unit_projection(t):\n",
    "    #     # return None\n",
    "    #     with torch.no_grad():\n",
    "    #         t.set_(t / torch.norm(t))\n",
    "\n",
    "    def fit_submodels(self, Xtrain: np.array, Ytrain: np.array):\n",
    "        # record meta data\n",
    "        self.classes = np.array(np.unique(Ytrain), dtype=np.long)\n",
    "\n",
    "        # fit all submodels in the ensemble\n",
    "        Ytrain = np.array(Ytrain, dtype=np.long)\n",
    "        self.models = []\n",
    "        for Classifier in self.classifiers:\n",
    "            model = Classifier(Xtrain, Ytrain)\n",
    "            model.fit()\n",
    "            self.models.append(model)\n",
    "\n",
    "        # init parameters\n",
    "        self.parameters = torch.rand((len(self.classifiers), len(self.classes)),\n",
    "                                     dtype=torch.float64, requires_grad=True, device=self.device)\n",
    "\n",
    "    def forward(self, X: np.array):\n",
    "        predictions = []\n",
    "        for classifier in self.models:\n",
    "            predictions.append(classifier.all_p_values(X))\n",
    "        P = torch.tensor(predictions).to(self.device).permute(1,0,2)\n",
    "        r = torch.sum(torch.mul(P, self.parameters**2), axis=1)\n",
    "        phat = torch.mul(r, 1/torch.sum(self.parameters**2, axis=0))\n",
    "        return phat\n",
    "\n",
    "    def all_p_values(self, X):\n",
    "        with torch.no_grad():\n",
    "            return self.forward(X)\n",
    "\n",
    "    def acc(self, Xtest: np.array, Ytest: np.array):\n",
    "        Ytest = torch.tensor(Ytest, dtype=torch.float64, device=self.device)\n",
    "\n",
    "        correct = torch.sum(Ytest==torch.argmax(self.all_p_values(Xtest), axis=1))\n",
    "        total   = Ytest.shape[0]\n",
    "        return correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "iteration:0 loss=0.15753015339097498 acc=0.98333340883255\n",
      "iteration:10 loss=0.1566190916004199 acc=0.98333340883255\n",
      "iteration:20 loss=0.1556610359774231 acc=0.98333340883255\n",
      "iteration:30 loss=0.15462606269035878 acc=0.98333340883255\n",
      "iteration:40 loss=0.15369982115595332 acc=0.98333340883255\n",
      "iteration:50 loss=0.1528504485122218 acc=0.98333340883255\n",
      "iteration:60 loss=0.15203094828649402 acc=0.98333340883255\n",
      "iteration:70 loss=0.1512148623322691 acc=0.98333340883255\n",
      "iteration:80 loss=0.15039476141225988 acc=0.98333340883255\n",
      "iteration:90 loss=0.1495732640448336 acc=0.98333340883255\n",
      "iteration:90 loss=0.1487553373102846 acc=0.98333340883255\n",
      "iteration:100 loss=0.1479443406334794 acc=0.98333340883255\n",
      "iteration:110 loss=0.14714099491201316 acc=0.98333340883255\n",
      "iteration:120 loss=0.14634379931364708 acc=0.98333340883255\n",
      "iteration:130 loss=0.1455498940361815 acc=0.98333340883255\n",
      "iteration:140 loss=0.1447559053215806 acc=0.98333340883255\n",
      "iteration:150 loss=0.14395861158009032 acc=0.98333340883255\n",
      "iteration:160 loss=0.14315539079631767 acc=0.98333340883255\n",
      "iteration:170 loss=0.14234445327705397 acc=0.98333340883255\n",
      "iteration:180 loss=0.14152488847948422 acc=0.9666666984558105\n",
      "iteration:180 loss=0.14069657477035707 acc=0.9666666984558105\n",
      "iteration:190 loss=0.13986001212841936 acc=0.9666666984558105\n",
      "iteration:200 loss=0.13901613531059115 acc=0.9666666984558105\n",
      "iteration:210 loss=0.1381661507383827 acc=0.9666666984558105\n",
      "iteration:220 loss=0.1373114209308816 acc=0.9666666984558105\n",
      "iteration:230 loss=0.13645340235924264 acc=0.9666666984558105\n",
      "iteration:240 loss=0.13559362979927125 acc=0.9666666984558105\n",
      "iteration:250 loss=0.1347337331399638 acc=0.9666666984558105\n",
      "iteration:260 loss=0.13387546999804908 acc=0.9666666984558105\n",
      "iteration:270 loss=0.13302075800801672 acc=0.9666666984558105\n",
      "iteration:270 loss=0.13217169338134457 acc=0.9666666984558105\n",
      "iteration:280 loss=0.13133054669997682 acc=0.9666666984558105\n",
      "iteration:290 loss=0.13049973233300957 acc=0.9666666984558105\n",
      "iteration:300 loss=0.12968175346182606 acc=0.9666666984558105\n",
      "iteration:310 loss=0.12887912940074092 acc=0.9666666984558105\n",
      "iteration:320 loss=0.12809431479806557 acc=0.9666666984558105\n",
      "iteration:330 loss=0.12732962097498693 acc=0.9666666984558105\n",
      "iteration:340 loss=0.12658714828253867 acc=0.9666666984558105\n",
      "iteration:350 loss=0.1258687355597842 acc=0.9666666984558105\n",
      "iteration:360 loss=0.12517592937017513 acc=0.9666666984558105\n",
      "iteration:360 loss=0.1245099724260362 acc=0.9666666984558105\n",
      "iteration:370 loss=0.12387180803587733 acc=0.9666666984558105\n",
      "iteration:380 loss=0.123262095850936 acc=0.9666666984558105\n",
      "iteration:390 loss=0.12268123375637507 acc=0.9666666984558105\n",
      "iteration:400 loss=0.12212938136011282 acc=0.9666666984558105\n",
      "iteration:410 loss=0.12160648190918687 acc=0.9666666984558105\n",
      "iteration:420 loss=0.1211122812063477 acc=0.9666666984558105\n",
      "iteration:430 loss=0.12064634375802596 acc=0.9666666984558105\n",
      "iteration:440 loss=0.12020806757429632 acc=0.9666666984558105\n",
      "iteration:450 loss=0.11979669953926603 acc=0.9666666984558105\n",
      "iteration:450 loss=0.1194113530583081 acc=0.9666666984558105\n",
      "iteration:460 loss=0.11905102893072154 acc=0.9666666984558105\n",
      "iteration:470 loss=0.11871463936891131 acc=0.9666666984558105\n",
      "iteration:480 loss=0.1184010340906823 acc=0.9666666984558105\n",
      "iteration:490 loss=0.11810902670407343 acc=0.9666666984558105\n",
      "iteration:500 loss=0.1178374193401022 acc=0.9666666984558105\n",
      "iteration:510 loss=0.11758502370221734 acc=0.9666666984558105\n",
      "iteration:520 loss=0.11735067730876242 acc=0.9666666984558105\n",
      "iteration:530 loss=0.11713325453363167 acc=0.9666666984558105\n",
      "iteration:540 loss=0.116931672886827 acc=0.9666666984558105\n",
      "iteration:540 loss=0.11674489562323606 acc=0.9666666984558105\n",
      "iteration:550 loss=0.1165719320934063 acc=0.9666666984558105\n",
      "iteration:560 loss=0.11641183721785728 acc=0.9666666984558105\n",
      "iteration:570 loss=0.11626371113425038 acc=0.9666666984558105\n",
      "iteration:580 loss=0.11612669956024141 acc=0.9666666984558105\n",
      "iteration:590 loss=0.11599999488683958 acc=0.9666666984558105\n",
      "iteration:600 loss=0.11588283760485756 acc=0.9666666984558105\n",
      "iteration:610 loss=0.11577451745936579 acc=0.9666666984558105\n",
      "iteration:620 loss=0.11567437374756269 acc=0.9666666984558105\n",
      "iteration:630 loss=0.11558179438418074 acc=0.9666666984558105\n",
      "iteration:630 loss=0.11549621366986527 acc=0.9666666984558105\n",
      "iteration:640 loss=0.11541710900896282 acc=0.9666666984558105\n",
      "iteration:650 loss=0.11534399704395466 acc=0.9666666984558105\n",
      "iteration:660 loss=0.1152764297511631 acc=0.9666666984558105\n",
      "iteration:670 loss=0.115213990970463 acc=0.9666666984558105\n",
      "iteration:680 loss=0.1151562936578797 acc=0.9666666984558105\n",
      "iteration:690 loss=0.11510297791865018 acc=0.9666666984558105\n",
      "iteration:700 loss=0.11505370966947641 acc=0.9666666984558105\n",
      "iteration:710 loss=0.11500817964701095 acc=0.9666666984558105\n",
      "iteration:720 loss=0.11496610245087464 acc=0.9666666984558105\n",
      "iteration:720 loss=0.11492721537673456 acc=0.9666666984558105\n",
      "iteration:730 loss=0.11489127692448739 acc=0.9666666984558105\n",
      "iteration:740 loss=0.11485806501109472 acc=0.9666666984558105\n",
      "iteration:750 loss=0.11482737503179606 acc=0.9666666984558105\n",
      "iteration:760 loss=0.11479901796716015 acc=0.9666666984558105\n",
      "iteration:770 loss=0.11477281871884983 acc=0.9666666984558105\n",
      "iteration:780 loss=0.11474861478791716 acc=0.9666666984558105\n",
      "iteration:790 loss=0.1147262553144309 acc=0.9666666984558105\n",
      "iteration:800 loss=0.11470560040883526 acc=0.9666666984558105\n",
      "iteration:810 loss=0.11468652064976972 acc=0.9666666984558105\n",
      "iteration:810 loss=0.11466889661264837 acc=0.9666666984558105\n",
      "iteration:820 loss=0.11465261832492364 acc=0.9666666984558105\n",
      "iteration:830 loss=0.1146375846018362 acc=0.9666666984558105\n",
      "iteration:840 loss=0.11462370227875014 acc=0.9666666984558105\n",
      "iteration:850 loss=0.1146108854027105 acc=0.9666666984558105\n",
      "iteration:860 loss=0.11459905446426752 acc=0.9666666984558105\n",
      "iteration:870 loss=0.1145881357392105 acc=0.9666666984558105\n",
      "iteration:880 loss=0.11457806077674616 acc=0.9666666984558105\n",
      "iteration:890 loss=0.1145687660297726 acc=0.9666666984558105\n",
      "iteration:900 loss=0.11456019258870284 acc=0.9666666984558105\n",
      "iteration:900 loss=0.11455228596300215 acc=0.9666666984558105\n",
      "iteration:910 loss=0.1145449958574466 acc=0.9666666984558105\n",
      "iteration:920 loss=0.11453827590923688 acc=0.9666666984558105\n",
      "iteration:930 loss=0.11453208337882496 acc=0.9666666984558105\n",
      "iteration:940 loss=0.11452637881167421 acc=0.9666666984558105\n",
      "iteration:950 loss=0.11452112570243948 acc=0.9666666984558105\n",
      "iteration:960 loss=0.11451629019398908 acc=0.9666666984558105\n",
      "iteration:970 loss=0.11451184083302288 acc=0.9666666984558105\n",
      "iteration:980 loss=0.11450774838717093 acc=0.9666666984558105\n",
      "iteration:990 loss=0.1145039857120991 acc=0.9666666984558105\n",
      "iteration:990 loss=0.1145005276468586 acc=0.9666666984558105\n",
      "iteration:1000 loss=0.11449735091413688 acc=0.9666666984558105\n",
      "iteration:1010 loss=0.11449443400845223 acc=0.9666666984558105\n",
      "iteration:1020 loss=0.11449175706627618 acc=0.9666666984558105\n",
      "iteration:1030 loss=0.11448930172307592 acc=0.9666666984558105\n",
      "iteration:1040 loss=0.11448705096950312 acc=0.9666666984558105\n",
      "iteration:1050 loss=0.11448498902046166 acc=0.9666666984558105\n",
      "iteration:1060 loss=0.11448310120693601 acc=0.9666666984558105\n",
      "iteration:1070 loss=0.11448137389346418 acc=0.9666666984558105\n",
      "iteration:1080 loss=0.11447979441699348 acc=0.9666666984558105\n",
      "iteration:1080 loss=0.11447835103816568 acc=0.9666666984558105\n",
      "iteration:1090 loss=0.11447703289516387 acc=0.9666666984558105\n",
      "iteration:1100 loss=0.11447582995287804 acc=0.9666666984558105\n",
      "iteration:1110 loss=0.11447473294481972 acc=0.9666666984558105\n",
      "iteration:1120 loss=0.11447373330992991 acc=0.9666666984558105\n",
      "iteration:1130 loss=0.11447282312945853 acc=0.9666666984558105\n",
      "iteration:1140 loss=0.1144719950696071 acc=0.9666666984558105\n",
      "iteration:1150 loss=0.11447124233383547 acc=0.9666666984558105\n",
      "iteration:1160 loss=0.11447055862567423 acc=0.9666666984558105\n",
      "iteration:1170 loss=0.11446993811990609 acc=0.9666666984558105\n",
      "iteration:1170 loss=0.11446937543819147 acc=0.9666666984558105\n",
      "iteration:1180 loss=0.11446886562510868 acc=0.9666666984558105\n",
      "iteration:1190 loss=0.1144684041219389 acc=0.9666666984558105\n",
      "iteration:1200 loss=0.11446798673761135 acc=0.9666666984558105\n",
      "iteration:1210 loss=0.11446760961813625 acc=0.9666666984558105\n",
      "iteration:1220 loss=0.11446726921689121 acc=0.9666666984558105\n",
      "iteration:1230 loss=0.1144669622680444 acc=0.9666666984558105\n",
      "iteration:1240 loss=0.11446668576440173 acc=0.9666666984558105\n",
      "iteration:1250 loss=0.11446643693959693 acc=0.9666666984558105\n",
      "iteration:1260 loss=0.11446621325339029 acc=0.9666666984558105\n",
      "iteration:1260 loss=0.11446601237832503 acc=0.9666666984558105\n",
      "iteration:1270 loss=0.11446583218621104 acc=0.9666666984558105\n",
      "iteration:1280 loss=0.11446567073366282 acc=0.9666666984558105\n",
      "iteration:1290 loss=0.1144655262468287 acc=0.9666666984558105\n",
      "iteration:1300 loss=0.11446539710613286 acc=0.9666666984558105\n",
      "iteration:1310 loss=0.11446528183207373 acc=0.9666666984558105\n",
      "iteration:1320 loss=0.11446517907287058 acc=0.9666666984558105\n",
      "iteration:1330 loss=0.11446508759420607 acc=0.9666666984558105\n",
      "iteration:1340 loss=0.11446500627073497 acc=0.9666666984558105\n",
      "iteration:1350 loss=0.11446493407866538 acc=0.9666666984558105\n",
      "iteration:1350 loss=0.11446487008867563 acc=0.9666666984558105\n",
      "iteration:1360 loss=0.11446481345867632 acc=0.9666666984558105\n",
      "iteration:1370 loss=0.11446476342631658 acc=0.9666666984558105\n",
      "iteration:1380 loss=0.11446471930148515 acc=0.9666666984558105\n",
      "iteration:1390 loss=0.1144646804592318 acc=0.9666666984558105\n",
      "iteration:1400 loss=0.11446464633349301 acc=0.9666666984558105\n",
      "iteration:1410 loss=0.11446461641179917 acc=0.9666666984558105\n",
      "iteration:1420 loss=0.11446459023088083 acc=0.9666666984558105\n",
      "iteration:1430 loss=0.11446456737289959 acc=0.9666666984558105\n",
      "iteration:1440 loss=0.11446454746196888 acc=0.9666666984558105\n",
      "iteration:1440 loss=0.11446453016071109 acc=0.9666666984558105\n",
      "iteration:1450 loss=0.11446451516675976 acc=0.9666666984558105\n",
      "iteration:1460 loss=0.11446450220927927 acc=0.9666666984558105\n",
      "iteration:1470 loss=0.11446449104567179 acc=0.9666666984558105\n",
      "iteration:1480 loss=0.1144644814586407 acc=0.9666666984558105\n",
      "iteration:1490 loss=0.11446447325369984 acc=0.9666666984558105\n",
      "iteration:1500 loss=0.11446446625710488 acc=0.9666666984558105\n",
      "iteration:1510 loss=0.11446446031409294 acc=0.9666666984558105\n",
      "iteration:1520 loss=0.11446445528728204 acc=0.9666666984558105\n",
      "iteration:1530 loss=0.11446445105511288 acc=0.9666666984558105\n",
      "iteration:1530 loss=0.11446444751028403 acc=0.9666666984558105\n",
      "iteration:1540 loss=0.11446444455820606 acc=0.9666666984558105\n",
      "iteration:1550 loss=0.11446444211554498 acc=0.9666666984558105\n",
      "iteration:1560 loss=0.1144644401089276 acc=0.9666666984558105\n",
      "iteration:1570 loss=0.11446443847384791 acc=0.9666666984558105\n",
      "iteration:1580 loss=0.11446443715376287 acc=0.9666666984558105\n",
      "iteration:1590 loss=0.11446443609932765 acc=0.9666666984558105\n",
      "iteration:1600 loss=0.11446443526770504 acc=0.9666666984558105\n",
      "iteration:1610 loss=0.1144644346218974 acc=0.9666666984558105\n",
      "iteration:1620 loss=0.11446443413008231 acc=0.9666666984558105\n",
      "iteration:1620 loss=0.1144644337649627 acc=0.9666666984558105\n",
      "iteration:1630 loss=0.11446443350316395 acc=0.9666666984558105\n",
      "iteration:1640 loss=0.11446443332470721 acc=0.9666666984558105\n",
      "iteration:1650 loss=0.11446443321257373 acc=0.9666666984558105\n",
      "iteration:1660 loss=0.11446443315235202 acc=0.9666666984558105\n",
      "iteration:1670 loss=0.11446443313194368 acc=0.9666666984558105\n",
      "iteration:1680 loss=0.11446443314129932 acc=0.9666666984558105\n",
      "iteration:1690 loss=0.11446443317216341 acc=0.9666666984558105\n",
      "iteration:1700 loss=0.11446443321782196 acc=0.9666666984558105\n",
      "iteration:1710 loss=0.11446443327285963 acc=0.9666666984558105\n",
      "iteration:1710 loss=0.11446443333294122 acc=0.9666666984558105\n",
      "iteration:1720 loss=0.1144644333946282 acc=0.9666666984558105\n",
      "iteration:1730 loss=0.11446443345523473 acc=0.9666666984558105\n",
      "iteration:1740 loss=0.11446443351271719 acc=0.9666666984558105\n",
      "iteration:1750 loss=0.11446443356558472 acc=0.9666666984558105\n",
      "iteration:1760 loss=0.11446443361281887 acc=0.9666666984558105\n",
      "iteration:1770 loss=0.11446443365379455 acc=0.9666666984558105\n",
      "iteration:1780 loss=0.11446443368820093 acc=0.9666666984558105\n",
      "iteration:1790 loss=0.11446443371596754 acc=0.9666666984558105\n",
      "iteration:1800 loss=0.11446443373720055 acc=0.9666666984558105\n",
      "After training P-hat\n",
      "tensor([[0.0000e+00, 2.0801e-05, 5.7684e-01],\n",
      "        [8.6754e-04, 9.3925e-13, 6.0407e-13],\n",
      "        [0.0000e+00, 3.9698e-05, 1.4581e-01],\n",
      "        [9.5085e-01, 3.4553e-12, 9.0150e-13],\n",
      "        [0.0000e+00, 3.2979e-02, 6.6192e-01],\n",
      "        [0.0000e+00, 1.2562e-03, 8.6686e-01],\n",
      "        [3.1944e-01, 3.0420e-14, 1.9662e-13],\n",
      "        [9.5085e-01, 4.5408e-14, 2.4025e-13],\n",
      "        [0.0000e+00, 7.7184e-05, 4.7057e-01],\n",
      "        [9.5085e-01, 2.6523e-13, 4.5330e-13],\n",
      "        [9.5085e-01, 8.8707e-14, 2.7112e-13],\n",
      "        [2.1358e-01, 2.3104e-13, 1.5121e-12],\n",
      "        [0.0000e+00, 7.9121e-01, 1.2824e-02],\n",
      "        [0.0000e+00, 2.9730e-07, 7.5928e-01],\n",
      "        [0.0000e+00, 2.2284e-12, 1.7322e-01],\n",
      "        [9.5085e-01, 1.2179e-13, 3.0598e-13],\n",
      "        [9.5085e-01, 9.3144e-12, 1.8167e-12],\n",
      "        [9.5085e-01, 2.7778e-13, 3.6970e-13],\n",
      "        [0.0000e+00, 4.4087e-01, 3.4398e-02],\n",
      "        [0.0000e+00, 3.8858e-01, 4.8218e-02],\n",
      "        [2.6161e-01, 5.4179e-14, 1.1435e-13],\n",
      "        [7.6969e-01, 5.4458e-10, 2.0365e-11],\n",
      "        [0.0000e+00, 7.9121e-01, 4.1652e-02],\n",
      "        [3.8831e-01, 3.8115e-12, 1.4910e-12],\n",
      "        [0.0000e+00, 7.7716e-16, 1.3384e-02],\n",
      "        [0.0000e+00, 9.3821e-01, 2.2749e-04],\n",
      "        [0.0000e+00, 7.0700e-13, 1.0385e-02],\n",
      "        [0.0000e+00, 3.8858e-01, 2.4845e-02],\n",
      "        [1.7402e-01, 2.2204e-16, 5.4401e-14],\n",
      "        [0.0000e+00, 1.2401e-01, 9.2137e-01],\n",
      "        [6.1927e-02, 5.4465e-49, 4.5519e-15],\n",
      "        [0.0000e+00, 2.4192e-13, 1.0454e-01],\n",
      "        [9.5085e-01, 4.7407e-14, 4.0656e-13],\n",
      "        [9.5085e-01, 1.1391e-11, 3.3081e-12],\n",
      "        [0.0000e+00, 4.7217e-08, 6.6192e-01],\n",
      "        [9.5085e-01, 6.1373e-13, 5.1958e-13],\n",
      "        [0.0000e+00, 1.1932e-06, 3.1796e-01],\n",
      "        [0.0000e+00, 9.3821e-01, 6.1163e-03],\n",
      "        [0.0000e+00, 9.3821e-01, 1.6453e-03],\n",
      "        [0.0000e+00, 1.4055e-01, 1.2443e-04],\n",
      "        [0.0000e+00, 1.1815e-04, 4.1194e-01],\n",
      "        [0.0000e+00, 2.5983e-04, 7.0909e-01],\n",
      "        [0.0000e+00, 1.4055e-01, 4.4015e-01],\n",
      "        [0.0000e+00, 3.8858e-01, 3.3892e-01],\n",
      "        [6.6329e-01, 8.5636e-11, 4.8899e-12],\n",
      "        [0.0000e+00, 9.3821e-01, 9.4319e-04],\n",
      "        [0.0000e+00, 2.8448e-06, 7.0909e-01],\n",
      "        [0.0000e+00, 2.6504e-01, 5.3873e-01],\n",
      "        [9.9341e-01, 1.2503e-12, 9.1416e-13],\n",
      "        [0.0000e+00, 6.3555e-01, 9.3868e-02],\n",
      "        [0.0000e+00, 8.6919e-01, 4.6470e-03],\n",
      "        [0.0000e+00, 1.4055e-01, 5.7684e-01],\n",
      "        [0.0000e+00, 5.6439e-01, 5.3236e-02],\n",
      "        [3.1944e-01, 1.3878e-14, 8.6819e-14],\n",
      "        [5.6134e-01, 2.9088e-13, 8.2956e-13],\n",
      "        [8.7067e-01, 3.6290e-12, 1.1746e-12],\n",
      "        [0.0000e+00, 8.6557e-03, 7.0909e-01],\n",
      "        [0.0000e+00, 6.3555e-01, 1.8982e-03],\n",
      "        [0.0000e+00, 1.3882e-14, 1.7363e-02],\n",
      "        [9.5085e-01, 4.0268e-13, 5.7188e-13]], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "True Label\n",
      "[2 1 0 2 0 2 0 1 1 1 2 1 1 1 1 0 1 1 0 0 2 1 0 0 2 0 0 1 1 0 2 1 0 2 2 1 0\n",
      " 1 1 1 2 0 2 0 0 1 2 2 2 2 1 2 1 1 2 2 2 2 1 2 1 0 2 1 1 1 1 2 0 0 2 1 0 0\n",
      " 1 0 2 1 0 1 2 1 0 2 2 2 2 0 0 2]\n",
      "train ACC=0.9666666984558105\n",
      "test  ACC=0.9666666984558105\n"
     ]
    }
   ],
   "source": [
    "# model definition\n",
    "model=Weighted_avg_ensemble(device='cuda')\n",
    "model.fit_submodels(Xtrain, Ytrain)\n",
    "\n",
    "# convert labels to onehot encoding\n",
    "Ytrain = np.array(Ytrain, dtype=np.long)\n",
    "Ytrain_onehot = onehot(Ytrain, np.unique(Ytrain).shape[0]).to(device)\n",
    "\n",
    "# Hyperparam\n",
    "batch_size = 10\n",
    "optimizer  = torch.optim.Adam([model.parameters], lr=1e-2)\n",
    "lossfn     = torch.nn.functional.mse_loss\n",
    "epochs     = 20\n",
    "\n",
    "# training loop\n",
    "# training_iteration = (Xtrain.shape[0] // batch_size + 1) * epochs\n",
    "for i in range(epochs):\n",
    "    for j in range(batch_size):\n",
    "\n",
    "        # get batch\n",
    "        x = Xtrain[j:min(j+10,len(Xtrain))]\n",
    "        y = Ytrain[j:min(j+10,len(Xtrain))]\n",
    "\n",
    "        # forward pass and backward pass\n",
    "        optimizer.zero_grad()\n",
    "        yhat = model.forward(Xtrain)\n",
    "        loss = lossfn(Ytrain_onehot, yhat)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        print(f\"iteration:{i*len(Xtrain)+j*batch_size} loss={loss.item()} acc={model.acc(Xtest, Ytest)}\")\n",
    "\n",
    "print(\"After training P-hat\")\n",
    "print(model.all_p_values(Xtest))\n",
    "print(\"True Label\")\n",
    "print(Ytrain)\n",
    "print(f\"train ACC={model.acc(Xtrain, Ytrain)}\")\n",
    "print(f\"test  ACC={model.acc(Xtest, Ytest)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0.00\t0.00\t0.58\t\n0.00\t0.00\t0.00\t\n0.00\t0.00\t0.15\t\n0.95\t0.00\t0.00\t\n0.00\t0.03\t0.66\t\n0.00\t0.00\t0.87\t\n0.32\t0.00\t0.00\t\n0.95\t0.00\t0.00\t\n0.00\t0.00\t0.47\t\n0.95\t0.00\t0.00\t\n0.95\t0.00\t0.00\t\n0.21\t0.00\t0.00\t\n0.00\t0.79\t0.01\t\n0.00\t0.00\t0.76\t\n0.00\t0.00\t0.17\t\n0.95\t0.00\t0.00\t\n0.95\t0.00\t0.00\t\n0.95\t0.00\t0.00\t\n0.00\t0.44\t0.03\t\n0.00\t0.39\t0.05\t\n0.26\t0.00\t0.00\t\n0.77\t0.00\t0.00\t\n0.00\t0.79\t0.04\t\n0.39\t0.00\t0.00\t\n0.00\t0.00\t0.01\t\n0.00\t0.94\t0.00\t\n0.00\t0.00\t0.01\t\n0.00\t0.39\t0.02\t\n0.17\t0.00\t0.00\t\n0.00\t0.12\t0.92\t\n0.06\t0.00\t0.00\t\n0.00\t0.00\t0.10\t\n0.95\t0.00\t0.00\t\n0.95\t0.00\t0.00\t\n0.00\t0.00\t0.66\t\n0.95\t0.00\t0.00\t\n0.00\t0.00\t0.32\t\n0.00\t0.94\t0.01\t\n0.00\t0.94\t0.00\t\n0.00\t0.14\t0.00\t\n0.00\t0.00\t0.41\t\n0.00\t0.00\t0.71\t\n0.00\t0.14\t0.44\t\n0.00\t0.39\t0.34\t\n0.66\t0.00\t0.00\t\n0.00\t0.94\t0.00\t\n0.00\t0.00\t0.71\t\n0.00\t0.27\t0.54\t\n0.99\t0.00\t0.00\t\n0.00\t0.64\t0.09\t\n0.00\t0.87\t0.00\t\n0.00\t0.14\t0.58\t\n0.00\t0.56\t0.05\t\n0.32\t0.00\t0.00\t\n0.56\t0.00\t0.00\t\n0.87\t0.00\t0.00\t\n0.00\t0.01\t0.71\t\n0.00\t0.64\t0.00\t\n0.00\t0.00\t0.02\t\n0.95\t0.00\t0.00\t\n"
     ]
    }
   ],
   "source": [
    "l=to_numpy(model.all_p_values(Xtest)).tolist()\n",
    "for r in l:\n",
    "    for i in r:\n",
    "        print(f\"{i:.2f}\", end=\"\\t\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "After training P-hat\ntensor([2, 0, 2, 0, 2, 2, 0, 0, 2, 0, 0, 0, 1, 2, 2, 0, 0, 0, 1, 1, 0, 0, 1, 0,\n        2, 1, 2, 1, 0, 2, 0, 2, 0, 0, 2, 0, 2, 1, 1, 1, 2, 2, 2, 1, 0, 1, 2, 2,\n        0, 1, 1, 2, 1, 0, 0, 0, 2, 1, 2, 0], device='cuda:0')\nTrue Label\n[2. 0. 2. 0. 2. 2. 0. 0. 2. 0. 0. 0. 1. 2. 2. 0. 0. 0. 1. 1. 0. 0. 1. 0.\n 2. 1. 2. 1. 0. 2. 0. 2. 0. 0. 2. 0. 2. 1. 1. 1. 2. 2. 1. 1. 0. 1. 2. 2.\n 0. 1. 1. 1. 1. 0. 0. 0. 2. 1. 2. 0.]\ntest  ACC=0.9666666984558105\n"
     ]
    }
   ],
   "source": [
    "print(\"After training P-hat\")\n",
    "print(torch.argmax(model.all_p_values(Xtest), axis=1))\n",
    "print(\"True Label\")\n",
    "print(Ytest)\n",
    "print(f\"test  ACC={model.acc(Xtest, Ytest)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[ 6.6422e-01,  1.6387e-01,  8.3616e-01],\n",
       "        [ 7.2779e-06, -1.3576e-07, -1.6002e-04]], device='cuda:0',\n",
       "       dtype=torch.float64, requires_grad=True)"
      ]
     },
     "metadata": {},
     "execution_count": 75
    }
   ],
   "source": [
    "model.parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}